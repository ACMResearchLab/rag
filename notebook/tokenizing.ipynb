{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os   \n",
    "import glob\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shotomorisaki/Engineering/rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "PARENT_DIR = os.path.dirname(os.getcwd())\n",
    "SRC_DIR = os.path.join(PARENT_DIR, 'src')\n",
    "sys.path.append(SRC_DIR)\n",
    "\n",
    "DATA_DIR = os.path.join(PARENT_DIR, 'data')\n",
    "\n",
    "from analyzer import CSVAnalyzer\n",
    "from graph import Graph\n",
    "from gpt import GPT\n",
    "from CodeRetriever import CodeRetriever\n",
    "from tokenizer import Tokenize\n",
    "from CosineSimilarity import CosineSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT()\n",
    "tokenizer = Tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(SRC_DIR, '*/**'))\n",
    "english_translated_text = []\n",
    "tokenized_eng_transl = []\n",
    "\n",
    "# Assuming you have a query vector 'query_vector', you should initialize it with appropriate values\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        # Translate Python code to English\n",
    "        translated_text = gpt.translate_code(content)\n",
    "        english_translated_text.append(translated_text)\n",
    "        \n",
    "        # Tokenize translated English text\n",
    "        tokenized_text = tokenizer.tokenizer(translated_text)\n",
    "        tokenized_eng_transl.append(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130,)\n",
      "(55,)\n",
      "(175,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(tokenized_text)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m cos \u001b[38;5;241m=\u001b[39m CosineSimilarity(np\u001b[38;5;241m.\u001b[39marray(tokenized_text))\n\u001b[0;32m---> 11\u001b[0m context_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_eng_transl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39mcompute_similarity(context_vectors)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "english_translated_text = \"The given code is a Python implementation of BLEU (Bilingual Evaluation Understudy) and smooth-BLEU, which are metrics for evaluating the quality of machine translation outputs. The code includes functions for computing n-grams, calculating BLEU scores, and applying smoothing techniques as outlined in the paper by Chin-Yew Lin and Franz Josef Och (COLING 2004). The main function `compute_bleu` takes reference and translation corpora as input and returns the BLEU score, n-gram precisions, geometric mean of n-gram precisions, and brevity penalty. Additionally, there's a helper function `_bleu` which takes a reference sentence and a translated sentence as input, and returns the BLEU score for that particular pair of sentences. The code also includes a license and citation to specify the rights and credits related to the implementation.\"\n",
    "tokenized_text = tokenizer.tokenizer(english_translated_text)\n",
    "\n",
    "print(np.array(tokenized_eng_transl[0]).shape)\n",
    "print(np.array(tokenized_eng_transl[1]).shape)\n",
    "print(np.array(tokenized_text).shape)\n",
    "cos = CosineSimilarity(np.array(tokenized_text))\n",
    "\n",
    "context_vectors = np.array(tokenized_eng_transl)\n",
    "\n",
    "similarity_scores = cos.compute_similarity(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
